Kafka is a distributed, partitioned, replicated “commit log service”.

A message is considered “committed” when all in sync replicas for that partition have applied it to their log.
Only committed messages are ever given out to the consumer.
This means that the consumer need not worry about potentially seeing a message that could be lost if the leader fails. 
Producers, on the other hand, have the option of either waiting for the message to be committed or not,
depending on their preference for tradeoff between latency and durability. 
This preference is controlled by the request.required.acks setting that the producer uses.

Kafka是一个基于副本的高可靠的消息系统，在消息可用前，Kafka保证消息已经提交到足够的副本中（这个在min.insync.replicas中配置），
这种逻辑类似于Zookeeper的写操作(Leader写，然后指定个数的Follower完成同步）。
Kafka不同于Zookeeper，Zookeeper是可以在网络发生分区后，能够继续工作


Kafka之所以和其它绝大多数信息系统不同，是因为下面这几个为数不多的比较重要的设计决策：

Kafka在设计之时为就将持久化消息作为通常的使用情况进行了考虑。
主要的设计约束是吞吐量而不是功能。
有关哪些数据已经被使用了的状态信息保存为数据使用者（consumer）的一部分，而不是保存在服务器之上。
Kafka是一种显式的分布式系统。它假设，数据生产者（producer）、代理（brokers）和数据使用者（consumer）分散于多台机器之上。

设计的很好的磁盘结构往往可以和网络一样快？
顺序磁盘访问能够比随即内存访问还要快？使用操作系统缓存，预读。
数据被传输到OS内核的页面缓存中了，OS随后会将这些数据刷新到磁盘的。
此外我们添加了一条基于配置的刷新策略，允许用户对把数据刷新到物理磁盘的频率进行控制（每当接收到N条消息或者每过M秒），
从而可以为系统硬件崩溃时“处于危险之中”的数据在量上加个上限。

持久化队列可以按照通常的日志解决方案的样子构建，只是简单的文件读取和简单地向文件中添加内容。

零拷贝：
sendfile这个系统调用实现，OS直接将数据从页面缓存拷贝到NIC（即网卡）的缓冲区。
常见的用例是一个话题拥有多个消息使用者。采用零拷贝优化方案，数据只需拷贝到页面缓存中一次，
然后每次发送给使用者时都对它进行重复使用即可，而无须先保存到内存中，然后在阅读该消息时每次都需要将其拷贝到内核空间中。
如此一来，消息使用的速度就能接近网络连接的极限。


分片：
每个切片都是一篇有序的日志，但是各片之间没有全局的次序（这个有别于你可能包含在消息中的挂钟时间）。
把消息分配到特定的日志片段这是由写入者控制的，大部分使用者会通过用户ID等键值来进行分片。
分片可以把日志追加到不存在协作的片段之间，也可以使系统的吞吐量与Kafka聚簇大小成线性比例关系。
缺少跨分片的全局顺序是这个机制的局限性，但是我们不认为它是最主要的。
事实上，与日志的交互主要来源于成百上千个不同的流程，以致于对于它们的行为排一个总体的顺序是没什么意义的。
相反，我们可以确保的是我们提供的每个分片都是按顺序保留的。Kafka保证了追加到由单一发送者送出的特定分片会按照发送的顺序依次处理。
Each individual partition must fit on the servers that host it, but a topic may have many partitions so it can handle an arbitrary amount of data.
Second they act as the unit of parallelism—more on that in a bit.



复制：

一条消息只有被“in sync” list里的所有follower都从leader复制过去才会被认为已提交。
这样就避免了部分数据被写进了leader，还没来得及被任何follower复制就宕机了，而造成数据丢失（consumer无法消费这些数据）。
而对于producer而言，它可以选择是否等待消息commit，这可以通过 request.required.acks 来设置。
这种机制确保了只要“in sync” list有一个或以上的flollower，一条被commit的消息就不会丢失。

这里的复制机制即不是同步复制，也不是单纯的异步复制。事实上，同步复制要求“活着的”follower都复制完，
这条消息才会被认为commit，这种复制方式极大的影响了吞吐率（高吞吐率是Kafka非常重要的一个特性）。
而异步复制方式下，follower异步的从leader复制数据，数据只要被leader写入log就被认为已经commit，
这种情况下如果follwer都落后于leader，而leader突然宕机，
则会丢失数据。而Kafka的这种使用“in sync” list的方式则很好的均衡了确保数据不丢失以及吞吐率。
follower可以批量的从leader复制数据，这样极大的提高复制性能（批量写磁盘），
极大减少了follower与leader的差距（前文有说到，只要follower落后leader不太远，则被认为在“in sync” list里）。

上文说明了Kafka是如何做replication的，另外一个很重要的问题是当leader宕机了，
怎样在follower中选举出新的leader。因为follower可能落后许多或者crash了，
所以必须确保选择“最新”的follower作为新的leader。一个基本的原则就是，如果leader不在了，
新的leader必须拥有原来的leader commit的所有消息。这就需要作一个折衷，
如果leader在标明一条消息被commit前等待更多的follower确认，那在它die之后就有更多的follower可以作为新的leader，
但这也会造成吞吐率的下降。


offset：
为使用者、话题和分区的每种组合记录一个“最高水位标记”（high water mark）即可。因此，标示使用者状态所需的元数据总量实际上特别小。
在Kafka中，我们将该最高水位标记称为“偏移量”（offset），这么叫的原因将在实现细节部分讲解。（http://www.oschina.net/translate/kafka-design）
使用者可用故意回退（rewind）到以前的偏移量处，再次使用一遍以前使用过的数据。


consumer:

If all the consumer instances have the same consumer group, then this works just like a traditional queue balancing load over the consumers.

If all the consumer instances have different consumer groups, then this works like publish-subscribe and all messages are broadcast to all consumers.

Kafka only provides a total order over messages within a partition, not between different partitions in a topic.
Per-partition ordering combined with the ability to partition data by key is sufficient for most applications.
However, if you require a total order over messages this can be achieved with a topic that has only one partition,
though this will mean only one consumer process per consumer group.


使用特点：
显然，这是一个集群的发布/订阅系统，有如下几个特点

生产者是推数据(Push)，消费者是拉数据(Pull)。存在数据复用，在Linkin平均生产1条消息会被消费5.5次。

数据生产者和数据消费者的速度不对等，所以要把数据沉淀在Kafka内慢慢处理，Linkin一般在集群内放7天的数据。

性能上追求高吞吐，保证一定的延时性之内。这方面做了大量优化，包括没有全局hash，批量发送，跨数据中心压缩等等。

容错性上使用的“至少传输一次”的语义。不保证强一次，但避免最多传一次的情况。

集群中数据分区，保证单个数据消费者可以读到某话题(topic)的某子话题(例如某用户的数据)的所有数据，避免全局读数据

数据规范性，所有数据分为数百个话题，然后在数据的源头——生产者(Producer)这边就用Schema来规范数据，
这种理念使得后期的数据传输、序列化、压缩、消费都有了统一的规范，同时也解决了这个领域非常麻烦的数据版本不兼容问题——生产者一改代码，消费者就抓瞎。

用于监控，这个系统的威力在于，前面所有生产系统的数据流向，通过这个系统都能关联起来，用于日常的运营也好，用于数据审计，用于运维级别的监控也好都是神器啊！
