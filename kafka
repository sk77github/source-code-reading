Kafka is a distributed, partitioned, replicated “commit log service”.

A message is considered “committed” when all in sync replicas for that partition have applied it to their log.
Only committed messages are ever given out to the consumer.
This means that the consumer need not worry about potentially seeing a message that could be lost if the leader fails. 
Producers, on the other hand, have the option of either waiting for the message to be committed or not,
depending on their preference for tradeoff between latency and durability. 
This preference is controlled by the request.required.acks setting that the producer uses.

Kafka是一个基于副本的高可靠的消息系统，在消息可用前，Kafka保证消息已经提交到足够的副本中（这个在min.insync.replicas中配置），
这种逻辑类似于Zookeeper的写操作(Leader写，然后指定个数的Follower完成同步）。
Kafka不同于Zookeeper，Zookeeper是可以在网络发生分区后，能够继续工作


Kafka之所以和其它绝大多数信息系统不同，是因为下面这几个为数不多的比较重要的设计决策：

Kafka在设计之时为就将持久化消息作为通常的使用情况进行了考虑。
主要的设计约束是吞吐量而不是功能。
有关哪些数据已经被使用了的状态信息保存为数据使用者（consumer）的一部分，而不是保存在服务器之上。
Kafka是一种显式的分布式系统。它假设，数据生产者（producer）、代理（brokers）和数据使用者（consumer）分散于多台机器之上。

设计的很好的磁盘结构往往可以和网络一样快？
顺序磁盘访问能够比随即内存访问还要快？使用操作系统缓存，预读



复制：

一条消息只有被“in sync” list里的所有follower都从leader复制过去才会被认为已提交。
这样就避免了部分数据被写进了leader，还没来得及被任何follower复制就宕机了，而造成数据丢失（consumer无法消费这些数据）。
而对于producer而言，它可以选择是否等待消息commit，这可以通过 request.required.acks 来设置。
这种机制确保了只要“in sync” list有一个或以上的flollower，一条被commit的消息就不会丢失。

这里的复制机制即不是同步复制，也不是单纯的异步复制。事实上，同步复制要求“活着的”follower都复制完，
这条消息才会被认为commit，这种复制方式极大的影响了吞吐率（高吞吐率是Kafka非常重要的一个特性）。
而异步复制方式下，follower异步的从leader复制数据，数据只要被leader写入log就被认为已经commit，
这种情况下如果follwer都落后于leader，而leader突然宕机，
则会丢失数据。而Kafka的这种使用“in sync” list的方式则很好的均衡了确保数据不丢失以及吞吐率。
follower可以批量的从leader复制数据，这样极大的提高复制性能（批量写磁盘），
极大减少了follower与leader的差距（前文有说到，只要follower落后leader不太远，则被认为在“in sync” list里）。

上文说明了Kafka是如何做replication的，另外一个很重要的问题是当leader宕机了，
怎样在follower中选举出新的leader。因为follower可能落后许多或者crash了，
所以必须确保选择“最新”的follower作为新的leader。一个基本的原则就是，如果leader不在了，
新的leader必须拥有原来的leader commit的所有消息。这就需要作一个折衷，
如果leader在标明一条消息被commit前等待更多的follower确认，那在它die之后就有更多的follower可以作为新的leader，
但这也会造成吞吐率的下降。



使用特点：
显然，这是一个集群的发布/订阅系统，有如下几个特点

生产者是推数据(Push)，消费者是拉数据(Pull)。存在数据复用，在Linkin平均生产1条消息会被消费5.5次。

数据生产者和数据消费者的速度不对等，所以要把数据沉淀在Kafka内慢慢处理，Linkin一般在集群内放7天的数据。

性能上追求高吞吐，保证一定的延时性之内。这方面做了大量优化，包括没有全局hash，批量发送，跨数据中心压缩等等。

容错性上使用的“至少传输一次”的语义。不保证强一次，但避免最多传一次的情况。

集群中数据分区，保证单个数据消费者可以读到某话题(topic)的某子话题(例如某用户的数据)的所有数据，避免全局读数据

数据规范性，所有数据分为数百个话题，然后在数据的源头——生产者(Producer)这边就用Schema来规范数据，
这种理念使得后期的数据传输、序列化、压缩、消费都有了统一的规范，同时也解决了这个领域非常麻烦的数据版本不兼容问题——生产者一改代码，消费者就抓瞎。

用于监控，这个系统的威力在于，前面所有生产系统的数据流向，通过这个系统都能关联起来，用于日常的运营也好，用于数据审计，用于运维级别的监控也好都是神器啊！
